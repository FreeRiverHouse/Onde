#!/usr/bin/env python3
"""
API Latency Analysis Tool (T279)

Analyzes API latency profiles generated by the autotrader.
Identifies bottlenecks, trends, and performance issues.

Usage:
    python analyze-api-latency.py [--profile PATH] [--format text|json]
"""

import json
import argparse
from pathlib import Path
from datetime import datetime


def load_profile(profile_path: str) -> dict:
    """Load latency profile from file."""
    path = Path(profile_path)
    if not path.exists():
        print(f"‚ùå Profile not found: {profile_path}")
        return None
    
    with open(path) as f:
        return json.load(f)


def format_ms(ms: float) -> str:
    """Format milliseconds with color indicators."""
    if ms < 200:
        return f"{ms:.0f}ms ‚úÖ"
    elif ms < 500:
        return f"{ms:.0f}ms"
    elif ms < 1000:
        return f"{ms:.0f}ms ‚ö†Ô∏è"
    else:
        return f"{ms:.0f}ms üî¥"


def analyze_profile(profile: dict) -> dict:
    """Analyze profile and identify issues."""
    results = {
        "summary": {},
        "bottlenecks": [],
        "recommendations": [],
        "by_category": {}
    }
    
    endpoints = profile.get("endpoints", {})
    if not endpoints:
        return results
    
    # Categorize endpoints
    categories = {
        "kalshi_api": [],
        "external_prices": [],
        "external_data": [],
        "errors": []
    }
    
    for endpoint, stats in endpoints.items():
        if stats.get("count", 0) == 0:
            continue
        
        if endpoint.endswith("_error") or endpoint.endswith("_timeout") or endpoint.endswith("_failed"):
            categories["errors"].append((endpoint, stats))
        elif endpoint.startswith("ext_"):
            if "ohlc" in endpoint or "fear" in endpoint:
                categories["external_data"].append((endpoint, stats))
            else:
                categories["external_prices"].append((endpoint, stats))
        else:
            categories["kalshi_api"].append((endpoint, stats))
    
    # Calculate category totals
    for cat_name, cat_endpoints in categories.items():
        if not cat_endpoints:
            continue
        
        total_calls = sum(s.get("count", 0) for _, s in cat_endpoints)
        avg_latencies = [s.get("avg_ms", 0) for _, s in cat_endpoints if s.get("count", 0) > 0]
        
        results["by_category"][cat_name] = {
            "endpoints": len(cat_endpoints),
            "total_calls": total_calls,
            "avg_latency": sum(avg_latencies) / len(avg_latencies) if avg_latencies else 0
        }
    
    # Identify bottlenecks
    for endpoint, stats in endpoints.items():
        count = stats.get("count", 0)
        if count < 3:
            continue
        
        avg = stats.get("avg_ms", 0)
        p95 = stats.get("p95_ms", 0)
        max_lat = stats.get("max_ms", 0)
        
        # Slow average
        if avg > 1000:
            results["bottlenecks"].append({
                "endpoint": endpoint,
                "issue": "slow_avg",
                "severity": "high" if avg > 2000 else "medium",
                "details": f"Average {avg:.0f}ms exceeds 1000ms threshold",
                "metric": avg
            })
        
        # Slow P95
        if p95 > 2000:
            results["bottlenecks"].append({
                "endpoint": endpoint,
                "issue": "slow_p95",
                "severity": "high" if p95 > 3000 else "medium",
                "details": f"P95 {p95:.0f}ms exceeds 2000ms threshold",
                "metric": p95
            })
        
        # High variance
        if avg > 0 and p95 / avg > 3:
            results["bottlenecks"].append({
                "endpoint": endpoint,
                "issue": "high_variance",
                "severity": "medium",
                "details": f"P95/Avg ratio {p95/avg:.1f}x indicates inconsistent performance",
                "metric": p95 / avg
            })
        
        # Spike detection (max >> p95)
        if p95 > 0 and max_lat / p95 > 3:
            results["bottlenecks"].append({
                "endpoint": endpoint,
                "issue": "spikes",
                "severity": "low",
                "details": f"Max {max_lat:.0f}ms >> P95 {p95:.0f}ms indicates occasional spikes",
                "metric": max_lat
            })
    
    # Generate recommendations
    if results["bottlenecks"]:
        high_severity = [b for b in results["bottlenecks"] if b["severity"] == "high"]
        if high_severity:
            results["recommendations"].append(
                "üî¥ Critical: Some endpoints have very high latency. Consider:\n"
                "   - Increasing timeouts\n"
                "   - Adding caching\n"
                "   - Using alternative data sources"
            )
        
        # Check for external API issues
        ext_issues = [b for b in results["bottlenecks"] if "ext_" in b["endpoint"]]
        if ext_issues:
            results["recommendations"].append(
                "‚ö†Ô∏è External APIs showing latency issues:\n"
                "   - Consider using local OHLC cache more aggressively\n"
                "   - Add fallback endpoints\n"
                "   - Implement request coalescing"
            )
    
    # Calculate overall summary
    all_calls = sum(s.get("count", 0) for s in endpoints.values())
    all_avgs = [s.get("avg_ms", 0) for s in endpoints.values() if s.get("count", 0) > 0]
    
    results["summary"] = {
        "total_calls": all_calls,
        "total_endpoints": len([e for e in endpoints if endpoints[e].get("count", 0) > 0]),
        "overall_avg_ms": sum(all_avgs) / len(all_avgs) if all_avgs else 0,
        "bottleneck_count": len(results["bottlenecks"]),
        "profile_timestamp": profile.get("timestamp")
    }
    
    return results


def print_text_report(profile: dict, analysis: dict):
    """Print formatted text report."""
    print("\n" + "=" * 70)
    print("üìä API LATENCY ANALYSIS REPORT")
    print("=" * 70)
    
    # Summary
    summary = analysis["summary"]
    print(f"\nüìà SUMMARY")
    print(f"   Profile generated: {summary.get('profile_timestamp', 'Unknown')}")
    print(f"   Total API calls: {summary.get('total_calls', 0):,}")
    print(f"   Active endpoints: {summary.get('total_endpoints', 0)}")
    print(f"   Overall avg latency: {format_ms(summary.get('overall_avg_ms', 0))}")
    print(f"   Bottlenecks detected: {summary.get('bottleneck_count', 0)}")
    
    # By category
    print(f"\nüì¶ BY CATEGORY")
    for cat, stats in analysis.get("by_category", {}).items():
        print(f"   {cat}:")
        print(f"      Endpoints: {stats['endpoints']}, Calls: {stats['total_calls']}, Avg: {format_ms(stats['avg_latency'])}")
    
    # Detailed endpoint stats
    endpoints = profile.get("endpoints", {})
    if endpoints:
        print(f"\nüìã ENDPOINT DETAILS")
        print("-" * 70)
        print(f"{'Endpoint':<25} {'Calls':>6} {'Min':>8} {'Avg':>8} {'P95':>8} {'Max':>8}")
        print("-" * 70)
        
        # Sort by avg latency descending
        sorted_endpoints = sorted(
            [(e, s) for e, s in endpoints.items() if s.get("count", 0) > 0],
            key=lambda x: x[1].get("avg_ms", 0),
            reverse=True
        )
        
        for endpoint, stats in sorted_endpoints:
            print(f"{endpoint:<25} {stats['count']:>6} {stats['min_ms']:>7.0f}ms {stats['avg_ms']:>7.0f}ms "
                  f"{stats['p95_ms']:>7.0f}ms {stats['max_ms']:>7.0f}ms")
    
    # Bottlenecks
    if analysis["bottlenecks"]:
        print(f"\n‚ö†Ô∏è BOTTLENECKS")
        for b in sorted(analysis["bottlenecks"], key=lambda x: ("high", "medium", "low").index(x["severity"])):
            severity_icon = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}[b["severity"]]
            print(f"   {severity_icon} {b['endpoint']}: {b['issue']}")
            print(f"      {b['details']}")
    
    # Recommendations
    if analysis["recommendations"]:
        print(f"\nüí° RECOMMENDATIONS")
        for rec in analysis["recommendations"]:
            print(f"   {rec}")
    
    print("\n" + "=" * 70)


def main():
    parser = argparse.ArgumentParser(description="Analyze API latency profiles")
    parser.add_argument("--profile", default="scripts/kalshi-latency-profile.json",
                        help="Path to latency profile JSON")
    parser.add_argument("--format", choices=["text", "json"], default="text",
                        help="Output format")
    args = parser.parse_args()
    
    profile = load_profile(args.profile)
    if not profile:
        return 1
    
    analysis = analyze_profile(profile)
    
    if args.format == "json":
        print(json.dumps(analysis, indent=2))
    else:
        print_text_report(profile, analysis)
    
    return 0


if __name__ == "__main__":
    exit(main())
