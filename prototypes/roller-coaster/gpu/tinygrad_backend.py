"""
TinyGrad/Radeon GPU Backend
Port numpy operations to GPU for massive speedups
"""
import numpy as np
from typing import Optional, Tuple, List
from dataclasses import dataclass

# Try to import tinygrad, fallback to numpy
try:
    from tinygrad import Tensor, Device
    from tinygrad.nn import optim
    TINYGRAD_AVAILABLE = True
    print(f"ðŸŽ® TinyGrad available! Device: {Device.DEFAULT}")
except ImportError:
    TINYGRAD_AVAILABLE = False
    print("âš ï¸ TinyGrad not available, using NumPy fallback")

# Unified array type
if TINYGRAD_AVAILABLE:
    Array = Tensor
else:
    Array = np.ndarray

def to_gpu(arr: np.ndarray) -> Array:
    """Move numpy array to GPU"""
    if TINYGRAD_AVAILABLE:
        return Tensor(arr.astype(np.float32))
    return arr

def to_cpu(arr: Array) -> np.ndarray:
    """Move array back to CPU"""
    if TINYGRAD_AVAILABLE:
        return arr.numpy()
    return arr

# GPU-accelerated operations
class GPUOps:
    """
    GPU-accelerated vector operations
    Uses TinyGrad when available, NumPy fallback otherwise
    """
    
    @staticmethod
    def normalize(v: Array) -> Array:
        """Normalize vectors - GPU batched"""
        if TINYGRAD_AVAILABLE:
            norm = (v * v).sum(axis=-1, keepdim=True).sqrt()
            return v / norm.maximum(Tensor([1e-8]))
        else:
            norm = np.linalg.norm(v, axis=-1, keepdims=True)
            return np.where(norm > 1e-8, v / norm, v)
    
    @staticmethod
    def dot(a: Array, b: Array) -> Array:
        """Batched dot product"""
        if TINYGRAD_AVAILABLE:
            return (a * b).sum(axis=-1)
        else:
            return np.sum(a * b, axis=-1)
    
    @staticmethod
    def cross(a: Array, b: Array) -> Array:
        """Cross product - GPU version"""
        if TINYGRAD_AVAILABLE:
            # Manual cross product for tinygrad
            return Tensor.stack([
                a[:, 1] * b[:, 2] - a[:, 2] * b[:, 1],
                a[:, 2] * b[:, 0] - a[:, 0] * b[:, 2],
                a[:, 0] * b[:, 1] - a[:, 1] * b[:, 0]
            ], dim=-1)
        else:
            return np.cross(a, b)
    
    @staticmethod
    def lerp(a: Array, b: Array, t: float) -> Array:
        """Linear interpolation"""
        return a + (b - a) * t
    
    @staticmethod
    def catmull_rom_batch(points: Array, t_values: Array) -> Array:
        """
        Batched Catmull-Rom spline evaluation
        Computes ALL spline points in parallel on GPU
        """
        n = points.shape[0] - 3  # Number of segments
        num_t = t_values.shape[0]
        
        if TINYGRAD_AVAILABLE:
            # GPU batch computation
            all_results = []
            
            for i in range(n):
                p0, p1, p2, p3 = points[i], points[i+1], points[i+2], points[i+3]
                
                t = t_values
                t2 = t * t
                t3 = t2 * t
                
                # Catmull-Rom coefficients
                result = (
                    p1 * 2 +
                    (-p0 + p2) * t.reshape(-1, 1) +
                    (p0 * 2 - p1 * 5 + p2 * 4 - p3) * t2.reshape(-1, 1) +
                    (-p0 + p1 * 3 - p2 * 3 + p3) * t3.reshape(-1, 1)
                ) * 0.5
                
                all_results.append(result)
            
            return Tensor.cat(all_results, dim=0)
        else:
            # NumPy fallback
            all_results = []
            
            for i in range(n):
                p0, p1, p2, p3 = points[i], points[i+1], points[i+2], points[i+3]
                
                for t in t_values:
                    t2 = t * t
                    t3 = t2 * t
                    
                    result = 0.5 * (
                        2 * p1 +
                        (-p0 + p2) * t +
                        (2*p0 - 5*p1 + 4*p2 - p3) * t2 +
                        (-p0 + 3*p1 - 3*p2 + p3) * t3
                    )
                    all_results.append(result)
            
            return np.array(all_results)


class GPUPhysics:
    """
    GPU-accelerated physics simulation
    Computes entire ride in parallel
    """
    
    def __init__(self, mass: float = 5000.0, gravity: float = 9.81):
        self.mass = mass
        self.gravity = gravity
        
    def compute_velocities(self, heights: Array, initial_speed: float = 1.0) -> Array:
        """
        Compute velocities from conservation of energy
        vÂ² = v0Â² + 2g(h0 - h)
        
        GPU-perfect: independent computation at each point
        """
        if TINYGRAD_AVAILABLE:
            h0 = heights[0]
            v0_sq = Tensor([initial_speed ** 2])
            
            # vÂ² = v0Â² + 2g(h0 - h)
            v_sq = v0_sq + 2 * self.gravity * (h0 - heights)
            v_sq = v_sq.maximum(Tensor([0.01]))  # Minimum velocity
            
            return v_sq.sqrt()
        else:
            h0 = heights[0]
            v0_sq = initial_speed ** 2
            
            v_sq = v0_sq + 2 * self.gravity * (h0 - heights)
            v_sq = np.maximum(v_sq, 0.01)
            
            return np.sqrt(v_sq)
    
    def compute_g_forces(self, velocities: Array, radii: Array) -> Array:
        """
        Compute centripetal G-forces
        G = vÂ² / (r * g)
        
        GPU-parallel computation
        """
        if TINYGRAD_AVAILABLE:
            radii_safe = radii.maximum(Tensor([0.1]))
            return (velocities * velocities) / (radii_safe * self.gravity)
        else:
            radii_safe = np.maximum(radii, 0.1)
            return (velocities ** 2) / (radii_safe * self.gravity)
    
    def compute_curvature_radii(self, points: Array) -> Array:
        """
        Estimate radius of curvature at each point
        Uses Menger curvature from 3 consecutive points
        """
        n = points.shape[0]
        radii = []
        
        if TINYGRAD_AVAILABLE:
            points_np = points.numpy()
        else:
            points_np = points
            
        for i in range(1, n - 1):
            p0, p1, p2 = points_np[i-1], points_np[i], points_np[i+1]
            
            a = np.linalg.norm(p1 - p0)
            b = np.linalg.norm(p2 - p1)
            c = np.linalg.norm(p2 - p0)
            
            area = 0.5 * np.linalg.norm(np.cross(p1 - p0, p2 - p0))
            
            if area < 1e-8:
                radii.append(1000.0)  # Straight line
            else:
                radii.append((a * b * c) / (4 * area))
        
        # Pad ends
        radii = [radii[0]] + radii + [radii[-1]]
        
        if TINYGRAD_AVAILABLE:
            return Tensor(np.array(radii, dtype=np.float32))
        return np.array(radii)


class GPURenderer:
    """
    GPU-accelerated rendering helpers
    For Blender Python API integration
    """
    
    @staticmethod
    def generate_rail_mesh(track_points: Array, rail_width: float = 1.0,
                          rail_height: float = 0.1) -> Tuple[Array, Array]:
        """
        Generate rail geometry from track centerline
        Returns: (vertices, faces)
        
        GPU-accelerated vertex generation
        """
        if TINYGRAD_AVAILABLE:
            points = track_points.numpy()
        else:
            points = track_points
            
        n = len(points)
        vertices = []
        faces = []
        
        for i in range(n):
            # Compute local frame
            if i < n - 1:
                tangent = points[i+1] - points[i]
            else:
                tangent = points[i] - points[i-1]
            tangent = tangent / np.linalg.norm(tangent)
            
            up = np.array([0, 0, 1])
            right = np.cross(tangent, up)
            right = right / np.linalg.norm(right)
            up = np.cross(right, tangent)
            
            # Generate 4 vertices per point (box cross-section)
            p = points[i]
            w, h = rail_width / 2, rail_height / 2
            
            vertices.append(p + right * w + up * h)
            vertices.append(p - right * w + up * h)
            vertices.append(p - right * w - up * h)
            vertices.append(p + right * w - up * h)
            
            # Generate faces connecting to next segment
            if i < n - 1:
                base = i * 4
                next_base = (i + 1) * 4
                
                for j in range(4):
                    j_next = (j + 1) % 4
                    faces.append([
                        base + j, base + j_next,
                        next_base + j_next, next_base + j
                    ])
        
        vertices = np.array(vertices)
        faces = np.array(faces)
        
        if TINYGRAD_AVAILABLE:
            return Tensor(vertices.astype(np.float32)), faces
        return vertices, faces
    
    @staticmethod  
    def generate_support_positions(track_points: Array, 
                                   interval: float = 20.0) -> Array:
        """
        Generate support column positions
        """
        if TINYGRAD_AVAILABLE:
            points = track_points.numpy()
        else:
            points = track_points
            
        supports = []
        distance = 0.0
        last_support = 0.0
        
        for i in range(1, len(points)):
            seg_length = np.linalg.norm(points[i] - points[i-1])
            distance += seg_length
            
            if distance - last_support >= interval:
                # Support at this point, going down to ground
                supports.append({
                    'top': points[i].copy(),
                    'bottom': np.array([points[i][0], points[i][1], 0]),
                    'index': i
                })
                last_support = distance
        
        return supports


def benchmark_gpu():
    """Benchmark GPU vs CPU performance"""
    import time
    
    # Generate test data
    n_points = 10000
    points = np.random.randn(n_points, 3).astype(np.float32)
    
    # CPU benchmark
    start = time.time()
    for _ in range(100):
        norms = np.linalg.norm(points, axis=-1)
    cpu_time = time.time() - start
    
    print(f"CPU: {cpu_time:.3f}s for 100 iterations")
    
    if TINYGRAD_AVAILABLE:
        # GPU benchmark
        points_gpu = Tensor(points)
        
        # Warmup
        _ = (points_gpu * points_gpu).sum(axis=-1).sqrt()
        
        start = time.time()
        for _ in range(100):
            norms = (points_gpu * points_gpu).sum(axis=-1).sqrt()
            norms.realize()  # Force computation
        gpu_time = time.time() - start
        
        print(f"GPU: {gpu_time:.3f}s for 100 iterations")
        print(f"Speedup: {cpu_time / gpu_time:.1f}x")
    else:
        print("GPU benchmark skipped (TinyGrad not available)")


if __name__ == "__main__":
    benchmark_gpu()
