# BIBBIAv1 - AMD Radeon + ClawdBot + Modelli Open Source

> **Guida COMPLETA per far funzionare ClawdBot con GPU AMD Radeon su macOS**
> Versione 1.2 - 2026-02-01

---

## ðŸš¨ HANDOVER - LEGGI PRIMA DI TUTTO ðŸš¨

### Obiettivo del Progetto
**Far funzionare ClawdBot con modelli open source locali usando TUTTA la VRAM della AMD Radeon 7900 XT (20GB)** invece di dipendere da API cloud (Claude, GPT, etc.).

### Il Problema Che Abbiamo Avuto
Abbiamo passato ore cercando di usare **Ollama** con la GPU AMD su macOS. Vedevamo solo 4.6GB di "VRAM" e i modelli andavano lentissimi o in timeout.

### La Scoperta Fondamentale
**Quei 4.6GB NON erano VRAM - erano RAM!**

Ollama su macOS con AMD Radeon **GIRA SU CPU**, non su GPU. Ollama supporta SOLO Metal (Apple Silicon). Non c'Ã¨ modo di farlo funzionare su AMD su macOS.

### La Soluzione (GiÃ  Documentata!)
La soluzione era giÃ  scritta nel documento BIBBIA-RADEON originale: **TinyGrad con le variabili magiche `AMD=1 AMD_LLVM=1`**.

TinyGrad ha un driver userspace che bypassa completamente la mancanza di driver kernel AMD su macOS.

### Cosa Devi Fare (Step-by-Step)

**PASSO 1 - Verifica che la GPU sia collegata:**
```bash
system_profiler SPDisplaysDataType | grep -A10 "External GPU"
```
Deve mostrare "AMD Radeon RX 7900" o simile.

**PASSO 2 - Verifica prerequisiti:**
```bash
# Python 3.11 OBBLIGATORIO
/opt/homebrew/bin/python3.11 --version

# LLVM installato
brew list llvm

# tiktoken installato
/opt/homebrew/bin/python3.11 -c "import tiktoken; print('OK')"
```

**PASSO 3 - Verifica TinyGrad e GPU AMD:**
```bash
cd ~/tinygrad
DEBUG=2 AMD=1 /opt/homebrew/bin/python3.11 -c "from tinygrad import Device; print('GPU AMD OK')"
```
Deve mostrare `gfx1100` e `GPU AMD OK`.

**PASSO 4 - Lancia il server LLM:**
```bash
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --serve 11434
```
Il primo avvio Ã¨ LENTO (compila kernel LLVM). Aspetta.

**PASSO 5 - Test che funziona:**
```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:14b", "messages": [{"role": "user", "content": "Ciao!"}]}'
```

**PASSO 6 - Configura ClawdBot:**
Modifica `~/.clawdbot/clawdbot.json` (vedi sezione "Integrazione ClawdBot" sotto).

### Task Ancora da Completare

| Task | PrioritÃ  | Note |
|------|----------|------|
| âœ… Capire perchÃ© solo 4.6GB VRAM | FATTO | Era RAM, non VRAM - Ollama usa CPU |
| âœ… Documentare soluzione | FATTO | TinyGrad + AMD=1 AMD_LLVM=1 |
| â³ Testare qwen2.5:14b su TinyGrad | ALTA | Modello da 20GB per sfruttare VRAM |
| â³ Verificare tool calling | ALTA | ClawdBot richiede `tool_calls` array nativo |
| â³ Se tool calling non funziona | MEDIA | Implementare workaround parsing JSON |

### Errori Commessi (Per Non Ripeterli)

1. **Passato ore su Ollama** - Non funzionerÃ  MAI su AMD macOS. Usa TinyGrad.
2. **Non letto la documentazione esistente** - La soluzione era giÃ  in BIBBIA-RADEON.md
3. **Confuso VRAM con RAM** - Se vedi ~4-5GB con AMD su macOS, probabilmente Ã¨ CPU/RAM.
4. **Creato documenti multipli** - Ora tutto Ã¨ in questo singolo file BIBBIAv1.md.

### Contesto: PerchÃ© Questo Progetto Ãˆ Importante
L'obiettivo Ã¨ **indipendenza AI** - poter usare modelli potenti localmente senza dipendere da provider cloud. Con 20-24GB di VRAM della Radeon, possiamo far girare modelli 14B-32B che sono competitivi con molte API commerciali.

---

## ðŸ”¥ CONFIGURAZIONE eGPU - REGOLE FONDAMENTALI (2026-02-01)

### âš ï¸ IL CONTESTO CRITICO

Stiamo usando TinyGrad su Mac Apple Silicon (M1) con AMD Radeon 20-24GB collegata via **USB4/Thunderbolt**.

**NON esistono:**
- Driver AMD kernel
- ROCm
- Metal
- CUDA
- ReBAR

TinyGrad comunica con la GPU **interamente in userspace**, senza BAR reale.
La VRAM non Ã¨ memory-mapped: Ã¨ gestita come **heap remoto**, con trasferimenti chunked (MMIO + memcpy staging).

**NON parlare di "bypass BAR"**: il BAR non viene bypassato, viene **aggirato** con accessi espliciti userspace.

### âœ… MODELLO OBBLIGATORIO

**Qwen2.5-14B denso, quantizzato Q4**

| Criterio | Valore | Motivo |
|----------|--------|--------|
| Architettura | **DENSA** | NO MoE (routing dinamico = disastro su eGPU) |
| Quantizzazione | **Q4_K_M** | ~8GB pesi, lascia spazio per KV cache |
| VRAM stimata | ~16-18GB | Pesi + KV cache con context 2k |
| Parametri | 14B | Massimo modello denso sensato |

**NON usare:**
- âŒ `qwen2.5:14b` - Ãˆ MoE, pessimo su eGPU userspace
- âŒ `qwq:32b` - Troppo grande (18GB solo pesi)
- âŒ Modelli FP16/BF16 - Usano il doppio della memoria
- âŒ Modelli con paging dinamico

### âœ… CONFIGURAZIONE OBBLIGATORIA

```bash
# Comando per lanciare Qwen2.5-14B su AMD Radeon
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --max_context 2048 --serve 11434
```

| Parametro | Valore | Motivo |
|-----------|--------|--------|
| `max_context` | â‰¤ 2048 | KV cache grande = disastro (roundtrip USB/PCIe per ogni token) |
| `batch` | 1 | No parallelismo, Ã¨ un acceleratore remoto lento |
| `HALF` | 1 (default) | Attivazioni FP16 per risparmiare memoria |

### ðŸ”§ PATCH CRITICO: Float16 in nn/state.py

**QUESTO PATCH Ãˆ ESSENZIALE** per evitare garbage output con modelli Qwen.

Il patch in `tinygrad-fix/tinygrad/nn/state.py` aggiunge `.cast(dtypes.float16)` a tutte le conversioni GGML:

```python
# PRIMA (garbage output)
return (q_to_uint8(blocks[:,2:], 4).bitcast(dtypes.int8) - 8) * blocks[:,:2].bitcast(dtypes.float16).cast(dtypes.float32)

# DOPO (funziona!)
return ((q_to_uint8(blocks[:,2:], 4).bitcast(dtypes.int8) - 8) * blocks[:,:2].bitcast(dtypes.float16).cast(dtypes.float32)).cast(dtypes.float16)
```

**Per applicare:**
```bash
cp ~/Projects/tinygrad-fix/tinygrad/nn/state.py ~/tinygrad/tinygrad/nn/state.py
```

### ðŸ“Š STRATEGIA MEMORIA

| Aspetto | Configurazione | Motivo |
|---------|---------------|--------|
| VRAM | Arena statica ~19-20GB | Allocare all'avvio, mai rilasciare |
| Pesi | Caricati UNA sola volta | No offload dinamico |
| KV cache | Limitata, quantizzata se possibile | Ogni token = roundtrip USB/PCIe |
| Unified memory | NO | Non esiste su eGPU userspace |
| Paging | NO | Troppo lento |

**Tratta la GPU come acceleratore remoto lento, non come GPU PCIe nativa.**

### ðŸ§ª TESTATO E FUNZIONANTE (2026-02-01)

| Modello | VRAM | Output | Note |
|---------|------|--------|------|
| `qwen3:0.6b` | ~1GB | âœ… OK | Test veloce |
| `qwen2.5:7b` | ~4GB | âœ… OK | Buon compromesso |
| `qwen2.5:14b` | ~8GB | â³ Testing | Modello target |
| `qwen2.5:14b` | ~20GB | âš ï¸ MoE | Non raccomandato |

---

## INDICE

1. [TL;DR - La Soluzione](#tldr---la-soluzione)
2. [PerchÃ© Ollama NON Funziona](#perchÃ©-ollama-non-funziona)
3. [L'Hack Geniale: TinyGrad](#lhack-geniale-tinygrad)
4. [Setup Completo](#setup-completo)
5. [Modelli Disponibili](#modelli-disponibili)
6. [Raccomandazioni Grok per 20-24GB VRAM](#raccomandazioni-grok-per-20-24gb-vram)
7. [Integrazione ClawdBot](#integrazione-clawdbot)
8. [Tool Calling](#tool-calling)
9. [Troubleshooting](#troubleshooting)
10. [Comandi Utili](#comandi-utili)

---

## TL;DR - La Soluzione

```bash
# L'UNICO modo per usare AMD Radeon su macOS:
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --serve 11434
```

**Questo espone un server OpenAI-compatible su `http://localhost:11434/v1`**

---

## PerchÃ© Ollama NON Funziona

### Il Problema

| Cosa | Funziona? | PerchÃ© |
|------|-----------|--------|
| Ollama su macOS + AMD | âŒ NO | Ollama supporta SOLO Metal (Apple Silicon) |
| ROCm su macOS | âŒ NO | AMD supporta ROCm SOLO su Linux |
| Driver kernel AMD | âŒ NO | Apple non supporta GPU AMD dal 2020 |
| Metal con AMD | âŒ NO | Metal Ã¨ solo per Apple Silicon |

### Cosa Succedeva

Quando provavamo Ollama su macOS con AMD Radeon:
- **VRAM mostrata: 4.6GB** â†’ Era RAM, non VRAM!
- **Timeout continui** â†’ Girava su CPU
- **Performance scarse** â†’ CPU invece di GPU
- **Modelli grandi impossibili** â†’ OOM su CPU

**Ollama su macOS con AMD = gira su CPU, NON su GPU!**

---

## L'Hack Geniale: TinyGrad

### Le Variabili Magiche

```bash
AMD=1 AMD_LLVM=1
```

**MEMORIZZA QUESTO. Ãˆ la chiave per tutto.**

### Cosa Fanno

| Variabile | Cosa Fa | Senza |
|-----------|---------|-------|
| `AMD=1` | Forza backend AMD invece di Metal/CPU | Usa M1 GPU (Metal) |
| `AMD_LLVM=1` | Compila kernel con LLVM Homebrew | Cerca ROCm e FALLISCE |

### PerchÃ© Funziona

1. **TinyGrad ha un driver USERSPACE** - Bypassa totalmente la mancanza di driver kernel
2. **LLVM Homebrew** - PuÃ² generare codice AMD GCN/RDNA come ROCm
3. **PCIe diretto** - La GPU Ã¨ accessibile via Thunderbolt anche senza driver Apple

### Il Risultato

- **SENZA hack**: GPU AMD inutilizzabile su macOS
- **CON hack**: 20-24GB VRAM completamente disponibili!

---

## Setup Completo

### Hardware Supportato

| Componente | Dettagli |
|------------|----------|
| Mac | MacBook Pro M1/M4 (o successivo) |
| eGPU Enclosure | Razer Core X V2 (o Thunderbolt 3/4 compatibile) |
| GPU | AMD Radeon RX 7900 XTX (24GB) o 7900 XT (20GB) |
| Architettura | GFX1100 = RDNA3 |

### Prerequisiti

```bash
# 1. Python 3.11 (OBBLIGATORIO - non altre versioni!)
brew install python@3.11

# 2. LLVM per compilare kernel AMD
brew install llvm

# 3. tiktoken per tokenizer
/opt/homebrew/bin/python3.11 -m pip install tiktoken
```

### Installazione TinyGrad

```bash
cd ~
git clone https://github.com/tinygrad/tinygrad.git
cd tinygrad
```

### Verifica GPU

```bash
# Test che la GPU AMD sia riconosciuta
cd ~/tinygrad
DEBUG=2 AMD=1 /opt/homebrew/bin/python3.11 -c "from tinygrad import Device; print('GPU AMD OK')"
```

**Output atteso:**
```
am remote:0: mode1 reset
am remote:0: AM_SOC initialized
...firmware loading...
am remote:0: boot done
AMDDevice: opening 0 with target (11, 0, 0) arch gfx1100
GPU AMD OK
```

### Verifica Hardware

```bash
system_profiler SPDisplaysDataType | grep -A5 "External GPU"
```

---

## Modelli Disponibili

### Modelli Supportati da TinyGrad

| Modello | Size | VRAM | VelocitÃ  | Comando |
|---------|------|------|----------|---------|
| `qwen3:0.6b` | 0.6B | ~1GB | ~30 tok/s | `--model qwen3:0.6b` |
| `llama3.2:1b` | 1B | ~2GB | ~15 tok/s | `--model llama3.2:1b` |
| `llama3.2:1b-q4` | 1B | ~1GB | ~20 tok/s | `--model llama3.2:1b-q4` |
| `qwen3:1.7b` | 1.7B | ~3GB | ~12 tok/s | `--model qwen3:1.7b` |
| `llama3.2:3b` | 3B | ~4GB | ~10 tok/s | `--model llama3.2:3b` |
| `llama3.2:3b-f16` | 3B | ~6GB | ~8 tok/s | `--model llama3.2:3b-f16` |
| `qwen2.5:7b` | 7B | ~5GB | ~3 tok/s | `--model qwen2.5:7b` |
| `qwen3:8b` | 8B | ~8GB | ~5 tok/s | `--model qwen3:8b` |
| `llama3.1:8b` | 8B | ~8GB | ~5 tok/s | `--model llama3.1:8b` |
| **`qwen2.5:14b`** | **14B** | **~9GB** | **~2 tok/s** | `--model qwen2.5:14b` |
| `olmoe` | MoE | ~7GB | ~3 tok/s | `--model olmoe` |
| ~~`qwen3:30b-a3b`~~ | ~~30B MoE~~ | ~~~20GB~~ | ~~SCONSIGLIATO~~ | ~~MoE pessimo su eGPU~~ |

### Raccomandazione per VRAM

| VRAM Disponibile | Modello Consigliato |
|------------------|---------------------|
| 8GB | `llama3.1:8b` o `qwen3:8b` |
| 16GB | `llama3.1:8b` con context lungo |
| 20GB (7900 XT) | **`qwen2.5:14b`** |
| 24GB (7900 XTX) | **`qwen2.5:14b`** con max context |

---

## Raccomandazioni per eGPU (Aggiornato 2026-02-01)

### Modelli CONSIGLIATI per TinyGrad + AMD eGPU

**REGOLA FONDAMENTALE: Solo modelli DENSI, mai MoE!**

| Modello | VRAM Pesi | VRAM Totale | Note |
|---------|-----------|-------------|------|
| **Qwen2.5-14B Q4** | ~9GB | ~16-18GB | âœ… **RACCOMANDATO** - Denso, funziona! |
| **Qwen2.5-7B Q4** | ~5GB | ~8-10GB | âœ… Alternativa piÃ¹ leggera |
| LLaMA 3.1 8B Q4 | ~5GB | ~8-10GB | âœ… Testato, funziona |
| Qwen3 0.6B-8B | ~1-8GB | ~2-12GB | âœ… Densi, funzionano |

### NON USARE (Motivi Tecnici)

| Modello | Problema |
|---------|----------|
| âŒ `qwen3:30b-a3b` | **MoE** - Routing dinamico = disastro su eGPU |
| âŒ `qwq:32b` | Troppo grande (~18GB solo pesi) |
| âŒ Modelli FP16 | Usano il doppio della memoria |
| âŒ Modelli split | TinyGrad non supporta GGUF multi-file |
| âŒ Mixtral/OLMoE | **MoE** - Stessi problemi di routing |

### PerchÃ© NO MoE su eGPU?

Mixture of Experts (MoE) richiede:
1. **Routing dinamico** - Seleziona esperti diversi per ogni token
2. **Accessi memoria sparsi** - Pessimo su connessione USB/Thunderbolt
3. **Overhead comunicazione** - Ogni token = multiple roundtrip

Su GPU PCIe nativa: OK (alta bandwidth)
Su eGPU userspace: DISASTRO (latenza USB/TB domina)

### Framework Alternativi (se serve piÃ¹ scelta modelli)

| Framework | Pro | Contro |
|-----------|-----|--------|
| **TinyGrad** | Funziona su macOS AMD! | Pochi modelli |
| vLLM AMD | PiÃ¹ modelli, veloce | Solo Linux |
| MLC-LLM | Ottimizzato AMD | Solo Linux |
| llama.cpp ROCm | Max velocitÃ  | Solo Linux |

---

## Integrazione ClawdBot

### Lanciare Server TinyGrad

```bash
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --serve 11434
```

Questo espone API su `http://localhost:11434/v1/chat/completions`

### Configurazione ClawdBot

Modifica `~/.clawdbot/clawdbot.json`:

```json
{
  "agents": {
    "defaults": {
      "model": {
        "primary": "radeon/qwen2.5:14b",
        "fallbacks": ["radeon/llama3.1:8b"]
      }
    }
  },
  "models": {
    "mode": "merge",
    "providers": {
      "radeon": {
        "baseUrl": "http://localhost:11434/v1",
        "apiKey": "none",
        "api": "openai-completions",
        "models": [
          {
            "id": "qwen2.5:14b",
            "name": "Qwen3 30B (Radeon TinyGrad)",
            "reasoning": false,
            "input": ["text"],
            "cost": {"input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0},
            "contextWindow": 32768,
            "maxTokens": 8192
          },
          {
            "id": "llama3.1:8b",
            "name": "LLaMA 3.1 8B (Radeon TinyGrad)",
            "reasoning": false,
            "input": ["text"],
            "cost": {"input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0},
            "contextWindow": 32768,
            "maxTokens": 8192
          }
        ]
      }
    }
  }
}
```

### Test API

```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:14b",
    "messages": [{"role": "user", "content": "Ciao!"}],
    "stream": false
  }'
```

---

## Tool Calling

### Il Problema

ClawdBot richiede **tool calling nativo** - il modello deve rispondere con:

```json
{"tool_calls": [{"function": {"name": "calculator", "arguments": {"x": "5+3"}}}]}
```

NON con JSON nel content:
```json
{"content": "{\"name\": \"calculator\", \"arguments\": ...}"}
```

### Stato Attuale

| Framework | Tool Calling |
|-----------|--------------|
| TinyGrad | âš ï¸ DA VERIFICARE |
| Ollama | âœ… Supportato (ma non funziona su AMD macOS) |

### TODO

1. Testare se TinyGrad server supporta tool calling
2. Se non supporta, valutare workaround (parsing JSON dal content)

### Test Tool Calling

```bash
curl -s http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:14b",
    "messages": [{"role": "user", "content": "Calculate 5+3 using the calculator tool"}],
    "tools": [{
      "type": "function",
      "function": {
        "name": "calculator",
        "description": "Performs math calculations",
        "parameters": {
          "type": "object",
          "properties": {"expression": {"type": "string"}},
          "required": ["expression"]
        }
      }
    }]
  }' | python3 -c "
import sys,json
d = json.load(sys.stdin)
msg = d['choices'][0]['message']
print('Tool calls:', 'YES' if msg.get('tool_calls') else 'NO')
"
```

---

## Troubleshooting

### GPU Non Trovata

1. Verifica eGPU collegata via Thunderbolt
2. `system_profiler SPDisplaysDataType | grep -A5 "External GPU"`
3. Riavvia terminale

### "lock file in use"

Un altro processo sta usando la GPU:
```bash
pkill -f "python.*AMD=1"
```

### Lento al Primo Avvio

Normale - TinyGrad compila i kernel LLVM la prima volta. Successive run piÃ¹ veloci.

### Out of Memory

- Usa modello piÃ¹ piccolo (`llama3.2:1b`)
- Riduci context: `--max_context 2048`

### ClawdBot "context window too small"

ClawdBot richiede minimo 16k context. TinyGrad dovrebbe supportarlo di default con modelli grandi.

---

## Comandi Utili

### Variabili Ambiente

| Variabile | Valore | Descrizione |
|-----------|--------|-------------|
| `AMD` | `1` | Abilita backend AMD |
| `AMD_LLVM` | `1` | Usa LLVM homebrew |
| `PYTHONPATH` | `.` | Include moduli TinyGrad |
| `DEBUG` | `1-7` | VerbositÃ  debug |

### Script di Lancio

Crea `~/start-radeon-llm.sh`:

```bash
#!/bin/bash
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --serve 11434
```

```bash
chmod +x ~/start-radeon-llm.sh
```

### Path Importanti

| Path | Descrizione |
|------|-------------|
| `~/tinygrad` | Clone TinyGrad |
| `~/tinygrad/tinygrad/apps/llm.py` | Server LLM |
| `~/start-radeon-llm.sh` | Script avvio |
| `~/.clawdbot/clawdbot.json` | Config ClawdBot |

---

## Fonti

- [TinyGrad GitHub](https://github.com/tinygrad/tinygrad)
- [TinyGrad LLM App](https://github.com/tinygrad/tinygrad/blob/master/tinygrad/apps/llm.py)
- [AMD GPU Performance for LLM](https://valohai.com/blog/amd-gpu-performance-for-llm-inference/)
- [Ollama Tool Calling Docs](https://docs.ollama.com/capabilities/tool-calling)

---

## NVIDIA RTX 5060 Ti - NON FUNZIONA (Analisi 2026-02-01)

### Status: âŒ IN ATTESA FIRMWARE GSP â‰¥575

**La RTX 5060 Ti (Blackwell/GB206) NON FUNZIONA su macOS via eGPU.**

NÃ© con TinyGrad, nÃ© con nessun altro metodo. Serve firmware GSP â‰¥575 che **non Ã¨ ancora pubblico**.

---

### Il Problema

| Aspetto | Dettaglio |
|---------|-----------|
| **GPU** | NVIDIA RTX 5060 Ti 16GB (GB206/Blackwell) |
| **Connessione** | USB4/Thunderbolt via dock ADT-UT3G o Razer Core X |
| **Errore** | `GSP_INIT_DONE returns NV_ERR_TIMEOUT (result=101)` |
| **Causa** | Firmware GSP 570.144 non supporta Blackwell via eGPU |
| **Soluzione** | Aspettare firmware GSP â‰¥575 (non ancora nel repo pubblico) |

---

### Issue Aperti su TinyGrad

- **[tinygrad#14334](https://github.com/tinygrad/tinygrad/issues/14334)** - RTX 5060 Ti GSP init timeout on macOS M1
- **[tinygrad#14338](https://github.com/tinygrad/tinygrad/issues/14338)** - RTX 5060 Ti fails GSP initialization over USB4/Thunderbolt

### Conferma dal Team TinyGrad

**nimlgen** (membro team tinygrad) ha confermato:
> "5060ti is not supported in gsp 570.144. gsp needs to be updated to >=575"

---

### Sequenza Errore GSP

```
0.0s - GSP FMC/COT boots successfully
0.0s - RPC queues initialize (writePtr=1)
0.5s - Display Engine initialization starts (NOCAT type 2)
0.6s - FBFLCN error: UNRECOGNIZED_CLIENT -> HUBCLIENT_CE0 -> HUBCLIENT_VIP
4.6s - Internal timeout
4.6s - GSP_INIT_DONE returns NV_ERR_TIMEOUT (result=101)
```

---

### Firmware nel Repo Pubblico

```
nvidia/gb202/gsp/
â”œâ”€â”€ bootloader-570.144.bin
â”œâ”€â”€ fmc-570.144.bin
â””â”€â”€ (NO gsp-*.bin per Blackwell!)
```

Il firmware 575+ necessario **non esiste ancora** nel repo [NVIDIA/linux-firmware](https://github.com/NVIDIA/linux-firmware).

---

### Cosa NON Funziona

| Tentativo | Risultato |
|-----------|-----------|
| Registry keys (RMInitializeHeadless, etc.) | âŒ Stesso timeout |
| FMC regkeys (0x0 - 0xFF) | âŒ Stesso timeout |
| BDF values diversi | âŒ Stesso timeout |
| NV_NAK=1 (LLVM compiler) | âŒ Serve comunque GSP per init |
| Patch FRTS/WPR dinamici | âŒ Regressione |
| Firmware 575.64 | â³ Non disponibile pubblicamente |

---

### Arto Bendiken (@bendiken) - NON Ãˆ Riuscito

Tweet virale del 21 Nov 2025: ["Late night hacking on making an RTX 5060 Ti work..."](https://x.com/bendiken/status/1992139703232188771)

**ATTENZIONE:** Il tweet sembra mostrare successo, ma leggendo i commenti:

1. **"Late night hacking on MAKING it work"** - Stava PROVANDO, non aveva fatto funzionare
2. **Nessun follow-up "funziona!"** - Mai postato conferma di successo
3. **Suoi commenti:**
   - "Nobody's tested that yet that I've seen, though"
   - "Not at the moment. You'd want an RTX 30xx or newer"
4. **Libreria creata:** [artob/libegpu](https://github.com/artob/libegpu) - Solo per ENUMERARE eGPU, non per usarle

**Conclusione:** Nessuno ha fatto funzionare la RTX 5060 Ti via eGPU su Mac (M1 o M4).

---

### Cosa Funziona (RTX 30/40 series)

TinyGrad supporta **RTX 30xx e 40xx** via USB4/Thunderbolt con firmware 570.144.

| GPU | Architettura | Boot Path | Status |
|-----|--------------|-----------|--------|
| RTX 3090 | Ampere (GA102) | Standard GSP | âœ… Funziona |
| RTX 4090 | Ada (AD102) | Standard GSP | âœ… Funziona |
| **RTX 5060 Ti** | **Blackwell (GB206)** | **FMC/COT** | **âŒ Aspetta FW 575+** |
| RTX 5070 | Blackwell (GB205) | FMC/COT | âŒ Aspetta FW 575+ |
| RTX 5080 | Blackwell (GB203) | FMC/COT | âŒ Aspetta FW 575+ |
| RTX 5090 | Blackwell (GB202) | FMC/COT | âŒ Aspetta FW 575+ |

---

### Issue NVIDIA Correlati

- [NVIDIA/open-gpu-kernel-modules#979](https://github.com/NVIDIA/open-gpu-kernel-modules/issues/979) - RTX 5080 TB5 eGPU hard locks
- [NVIDIA/open-gpu-kernel-modules#981](https://github.com/NVIDIA/open-gpu-kernel-modules/pull/981) - TB eGPU detection improvements (CHIUSO)

---

### Prossimi Passi

1. **Monitorare** il repo [NVIDIA/linux-firmware](https://github.com/NVIDIA/linux-firmware) per firmware 575+
2. **Monitorare** issue [tinygrad#14338](https://github.com/tinygrad/tinygrad/issues/14338) per aggiornamenti
3. **Usare AMD RX 7900 XTX/XT** nel frattempo (funziona perfettamente!)

---

### Hardware Testato (NON Funzionante)

| Componente | Dettaglio |
|------------|-----------|
| Mac | MacBook Pro M1 / MacBook Pro M4 |
| GPU | NVIDIA RTX 5060 Ti 16GB |
| Dock | ADT-UT3G / Razer Core X V2 |
| Driver | TinyGPU (socket /tmp/tinygpu.sock OK) |
| Firmware | 570.144 (unico disponibile) |
| SIP | Disabilitato |

---

## ðŸ”¬ TEST SESSION 2026-02-01 (Claude Code)

### Obiettivo
Testare qwen2.5:14b sulla RX 7900 XT (20GB VRAM) con la config raccomandata da ChatGPT.

### Ambiente
- **GPU:** AMD Radeon RX 7900 XT (20GB VRAM) via Thunderbolt
- **Driver:** TinyGPU userspace driver
- **TinyGrad:** ~/tinygrad (con patch Float16 applicato)
- **Python:** 3.11

### Test Eseguiti

| Test | Risultato | Note |
|------|-----------|------|
| GPU Init | âœ… OK | `Device['AMD']` funziona |
| Allocazione 30GB | âœ… OK | Usa RAM come fallback oltre 20GB |
| qwen3:0.6b | âœ… OK | ~1GB, genera token |
| qwen2.5:7b | âš ï¸ Garbage | Carica ma output corrotto ("pÃµe..") |
| qwen2.5:14b load | âŒ OOM | Arriva a ~21GB poi fallisce |
| qwen2.5:14b realize=False | âœ… Parziale | 4 batch (200 tensors) poi OOM |
| llm.py server | âŒ Crash | Server termina dopo pochi secondi |

### Analisi Problema OOM

**Errore tipico:**
```
MemoryError: Failed to allocate memory. (total allocation size=0x8700000, current try=(4096, 4096))
```

**Causa:** Il modello qwen2.5:14b Q4_K_M (8.4GB file) richiede ~21GB durante realize:
- Pesi Q4: ~9GB
- De-quantizzazione temporanea: ~?GB
- KV cache (1024 ctx): ~?GB
- Totale > 20GB disponibili

**Tentativo di soluzione:**
- `max_context=256` â†’ stesso errore
- `realize=False` + batch realize â†’ arriva a 200 tensors poi OOM
- Server llm.py â†’ crash

### Conclusioni

1. **qwen2.5:14b NON entra nei 20GB** della RX 7900 XT con TinyGrad attuale
2. **qwen2.5:7b carica** ma ha output corrotto (possibile bug tokenizer)
3. **Modelli piccoli (0.6b-3b) funzionano** perfettamente
4. **Serve RX 7900 XTX (24GB)** o ottimizzazioni TinyGrad per il 14b

### Prossimi Passi

1. **Testare con meno context** (128 token) per vedere se entra
2. **Investigare output corrotto** su qwen2.5:7b
3. **Aprire issue TinyGrad** per OOM su modelli 14B+
4. **Considerare llama3.1:8b** come alternativa (testato funzionante 2026-01-28)

### Modelli Confermati Funzionanti

| Modello | VRAM | Output | VelocitÃ  |
|---------|------|--------|----------|
| qwen3:0.6b | ~1GB | âœ… OK | ~30 tok/s |
| llama3.1:8b Q4 | ~5GB | âœ… OK | ~0.64 tok/s |
| GPT-2 XL | ~6.5GB | âœ… OK | ~2.9 tok/s |

### Modelli NON Funzionanti

| Modello | Problema |
|---------|----------|
| qwen2.5:14b | OOM (>20GB) |
| qwen2.5:7b | Output garbage |
| qwq-32b | Troppo grande (18GB pesi) |

---

## Changelog

- **v1.3 (2026-02-01)**: Aggiunta sessione test Claude Code - qwen2.5:14b OOM, qwen2.5:7b garbage, analisi limiti VRAM
- **v1.2 (2026-02-01)**: Aggiunta sezione NVIDIA RTX 5060 Ti - analisi completa del fallimento, riferimenti issue, debunking tweet Arto Bendiken
- **v1.1 (2026-02-01)**: Aggiunta sezione HANDOVER dettagliata con step-by-step, errori commessi, task pendenti
- **v1.0 (2026-02-01)**: Consolidamento BIBBIA-RADEON + HANDOVER + CLAWDBOT-SETUP + raccomandazioni Grok

---

*Documento consolidato da: BIBBIA-RADEON.md, HANDOVER-CLAWDBOT-RADEON.md, CLAWDBOT-RADEON-SETUP.md*
*Questo Ã¨ l'UNICO documento di riferimento. Non creare altri file.*
