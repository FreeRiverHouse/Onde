# BIBBIAv1 - AMD Radeon + ClawdBot + Modelli Open Source

> **Guida COMPLETA per far funzionare ClawdBot con GPU AMD Radeon su macOS**
> Versione 1.2 - 2026-02-01

---

## üö® HANDOVER - LEGGI PRIMA DI TUTTO üö®

### Obiettivo del Progetto
**Far funzionare ClawdBot con modelli open source locali usando TUTTA la VRAM della AMD Radeon 7900 XT (20GB)** invece di dipendere da API cloud (Claude, GPT, etc.).

### Il Problema Che Abbiamo Avuto
Abbiamo passato ore cercando di usare **Ollama** con la GPU AMD su macOS. Vedevamo solo 4.6GB di "VRAM" e i modelli andavano lentissimi o in timeout.

### La Scoperta Fondamentale
**Quei 4.6GB NON erano VRAM - erano RAM!**

Ollama su macOS con AMD Radeon **GIRA SU CPU**, non su GPU. Ollama supporta SOLO Metal (Apple Silicon). Non c'√® modo di farlo funzionare su AMD su macOS.

### La Soluzione (Gi√† Documentata!)
La soluzione era gi√† scritta nel documento BIBBIA-RADEON originale: **TinyGrad con le variabili magiche `AMD=1 AMD_LLVM=1`**.

TinyGrad ha un driver userspace che bypassa completamente la mancanza di driver kernel AMD su macOS.

### Cosa Devi Fare (Step-by-Step)

**PASSO 1 - Verifica che la GPU sia collegata:**
```bash
system_profiler SPDisplaysDataType | grep -A10 "External GPU"
```
Deve mostrare "AMD Radeon RX 7900" o simile.

**PASSO 2 - Verifica prerequisiti:**
```bash
# Python 3.11 OBBLIGATORIO
/opt/homebrew/bin/python3.11 --version

# LLVM installato
brew list llvm

# tiktoken installato
/opt/homebrew/bin/python3.11 -c "import tiktoken; print('OK')"
```

**PASSO 3 - Verifica TinyGrad e GPU AMD:**
```bash
cd ~/tinygrad
DEBUG=2 AMD=1 /opt/homebrew/bin/python3.11 -c "from tinygrad import Device; print('GPU AMD OK')"
```
Deve mostrare `gfx1100` e `GPU AMD OK`.

**PASSO 4 - Lancia il server LLM:**
```bash
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --serve 11434
```
Il primo avvio √® LENTO (compila kernel LLVM). Aspetta.

**PASSO 5 - Test che funziona:**
```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:14b", "messages": [{"role": "user", "content": "Ciao!"}]}'
```

**PASSO 6 - Configura ClawdBot:**
Modifica `~/.clawdbot/clawdbot.json` (vedi sezione "Integrazione ClawdBot" sotto).

### Task Ancora da Completare

| Task | Priorit√† | Note |
|------|----------|------|
| ‚úÖ Capire perch√© solo 4.6GB VRAM | FATTO | Era RAM, non VRAM - Ollama usa CPU |
| ‚úÖ Documentare soluzione | FATTO | TinyGrad + AMD=1 AMD_LLVM=1 |
| ‚è≥ Testare qwen2.5:14b su TinyGrad | ALTA | Modello da 20GB per sfruttare VRAM |
| ‚è≥ Verificare tool calling | ALTA | ClawdBot richiede `tool_calls` array nativo |
| ‚è≥ Se tool calling non funziona | MEDIA | Implementare workaround parsing JSON |

### Errori Commessi (Per Non Ripeterli)

1. **Passato ore su Ollama** - Non funzioner√† MAI su AMD macOS. Usa TinyGrad.
2. **Non letto la documentazione esistente** - La soluzione era gi√† in BIBBIA-RADEON.md
3. **Confuso VRAM con RAM** - Se vedi ~4-5GB con AMD su macOS, probabilmente √® CPU/RAM.
4. **Creato documenti multipli** - Ora tutto √® in questo singolo file BIBBIAv1.md.

### Contesto: Perch√© Questo Progetto √à Importante
L'obiettivo √® **indipendenza AI** - poter usare modelli potenti localmente senza dipendere da provider cloud. Con 20-24GB di VRAM della Radeon, possiamo far girare modelli 14B-32B che sono competitivi con molte API commerciali.

---

## üî• CONFIGURAZIONE eGPU - REGOLE FONDAMENTALI (2026-02-01)

### ‚ö†Ô∏è IL CONTESTO CRITICO

Stiamo usando TinyGrad su Mac Apple Silicon (M1) con AMD Radeon 20-24GB collegata via **USB4/Thunderbolt**.

**NON esistono:**
- Driver AMD kernel
- ROCm
- Metal
- CUDA
- ReBAR

TinyGrad comunica con la GPU **interamente in userspace**, senza BAR reale.
La VRAM non √® memory-mapped: √® gestita come **heap remoto**, con trasferimenti chunked (MMIO + memcpy staging).

**NON parlare di "bypass BAR"**: il BAR non viene bypassato, viene **aggirato** con accessi espliciti userspace.

### ‚úÖ MODELLO OBBLIGATORIO

**Qwen2.5-14B denso, quantizzato Q4**

| Criterio | Valore | Motivo |
|----------|--------|--------|
| Architettura | **DENSA** | NO MoE (routing dinamico = disastro su eGPU) |
| Quantizzazione | **Q4_K_M** | ~8GB pesi, lascia spazio per KV cache |
| VRAM stimata | ~16-18GB | Pesi + KV cache con context 2k |
| Parametri | 14B | Massimo modello denso sensato |

**NON usare:**
- ‚ùå `qwen2.5:14b` - √à MoE, pessimo su eGPU userspace
- ‚ùå `qwq:32b` - Troppo grande (18GB solo pesi)
- ‚ùå Modelli FP16/BF16 - Usano il doppio della memoria
- ‚ùå Modelli con paging dinamico

### ‚úÖ CONFIGURAZIONE OBBLIGATORIA

```bash
# Comando per lanciare Qwen2.5-14B su AMD Radeon
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --max_context 2048 --serve 11434
```

| Parametro | Valore | Motivo |
|-----------|--------|--------|
| `max_context` | ‚â§ 2048 | KV cache grande = disastro (roundtrip USB/PCIe per ogni token) |
| `batch` | 1 | No parallelismo, √® un acceleratore remoto lento |
| `HALF` | 1 (default) | Attivazioni FP16 per risparmiare memoria |

### üîß PATCH CRITICO: Float16 in nn/state.py

**QUESTO PATCH √à ESSENZIALE** per evitare garbage output con modelli Qwen.

Il patch in `tinygrad-fix/tinygrad/nn/state.py` aggiunge `.cast(dtypes.float16)` a tutte le conversioni GGML:

```python
# PRIMA (garbage output)
return (q_to_uint8(blocks[:,2:], 4).bitcast(dtypes.int8) - 8) * blocks[:,:2].bitcast(dtypes.float16).cast(dtypes.float32)

# DOPO (funziona!)
return ((q_to_uint8(blocks[:,2:], 4).bitcast(dtypes.int8) - 8) * blocks[:,:2].bitcast(dtypes.float16).cast(dtypes.float32)).cast(dtypes.float16)
```

**Per applicare:**
```bash
cp ~/Projects/tinygrad-fix/tinygrad/nn/state.py ~/tinygrad/tinygrad/nn/state.py
```

### üìä STRATEGIA MEMORIA

| Aspetto | Configurazione | Motivo |
|---------|---------------|--------|
| VRAM | Arena statica ~19-20GB | Allocare all'avvio, mai rilasciare |
| Pesi | Caricati UNA sola volta | No offload dinamico |
| KV cache | Limitata, quantizzata se possibile | Ogni token = roundtrip USB/PCIe |
| Unified memory | NO | Non esiste su eGPU userspace |
| Paging | NO | Troppo lento |

**Tratta la GPU come acceleratore remoto lento, non come GPU PCIe nativa.**

### üß™ TESTATO E FUNZIONANTE (2026-02-01)

| Modello | VRAM | Output | Note |
|---------|------|--------|------|
| `qwen3:0.6b` | ~1GB | ‚úÖ OK | Test veloce |
| `qwen2.5:7b` | ~4GB | ‚úÖ OK | Buon compromesso |
| `qwen2.5:14b` | ~8GB | ‚è≥ Testing | Modello target |
| `qwen2.5:14b` | ~20GB | ‚ö†Ô∏è MoE | Non raccomandato |

---

## INDICE

1. [TL;DR - La Soluzione](#tldr---la-soluzione)
2. [Perch√© Ollama NON Funziona](#perch√©-ollama-non-funziona)
3. [L'Hack Geniale: TinyGrad](#lhack-geniale-tinygrad)
4. [Setup Completo](#setup-completo)
5. [Modelli Disponibili](#modelli-disponibili)
6. [Raccomandazioni Grok per 20-24GB VRAM](#raccomandazioni-grok-per-20-24gb-vram)
7. [Integrazione ClawdBot](#integrazione-clawdbot)
8. [Tool Calling](#tool-calling)
9. [Troubleshooting](#troubleshooting)
10. [Comandi Utili](#comandi-utili)

---

## TL;DR - La Soluzione

```bash
# L'UNICO modo per usare AMD Radeon su macOS:
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --serve 11434
```

**Questo espone un server OpenAI-compatible su `http://localhost:11434/v1`**

---

## Perch√© Ollama NON Funziona

### Il Problema

| Cosa | Funziona? | Perch√© |
|------|-----------|--------|
| Ollama su macOS + AMD | ‚ùå NO | Ollama supporta SOLO Metal (Apple Silicon) |
| ROCm su macOS | ‚ùå NO | AMD supporta ROCm SOLO su Linux |
| Driver kernel AMD | ‚ùå NO | Apple non supporta GPU AMD dal 2020 |
| Metal con AMD | ‚ùå NO | Metal √® solo per Apple Silicon |

### Cosa Succedeva

Quando provavamo Ollama su macOS con AMD Radeon:
- **VRAM mostrata: 4.6GB** ‚Üí Era RAM, non VRAM!
- **Timeout continui** ‚Üí Girava su CPU
- **Performance scarse** ‚Üí CPU invece di GPU
- **Modelli grandi impossibili** ‚Üí OOM su CPU

**Ollama su macOS con AMD = gira su CPU, NON su GPU!**

---

## L'Hack Geniale: TinyGrad

### Le Variabili Magiche

```bash
AMD=1 AMD_LLVM=1
```

**MEMORIZZA QUESTO. √à la chiave per tutto.**

### Cosa Fanno

| Variabile | Cosa Fa | Senza |
|-----------|---------|-------|
| `AMD=1` | Forza backend AMD invece di Metal/CPU | Usa M1 GPU (Metal) |
| `AMD_LLVM=1` | Compila kernel con LLVM Homebrew | Cerca ROCm e FALLISCE |

### Perch√© Funziona

1. **TinyGrad ha un driver USERSPACE** - Bypassa totalmente la mancanza di driver kernel
2. **LLVM Homebrew** - Pu√≤ generare codice AMD GCN/RDNA come ROCm
3. **PCIe diretto** - La GPU √® accessibile via Thunderbolt anche senza driver Apple

### Il Risultato

- **SENZA hack**: GPU AMD inutilizzabile su macOS
- **CON hack**: 20-24GB VRAM completamente disponibili!

---

## Setup Completo

### Hardware Supportato

| Componente | Dettagli |
|------------|----------|
| Mac | MacBook Pro M1/M4 (o successivo) |
| eGPU Enclosure | Razer Core X V2 (o Thunderbolt 3/4 compatibile) |
| GPU | AMD Radeon RX 7900 XTX (24GB) o 7900 XT (20GB) |
| Architettura | GFX1100 = RDNA3 |

### Prerequisiti

```bash
# 1. Python 3.11 (OBBLIGATORIO - non altre versioni!)
brew install python@3.11

# 2. LLVM per compilare kernel AMD
brew install llvm

# 3. tiktoken per tokenizer
/opt/homebrew/bin/python3.11 -m pip install tiktoken
```

### Installazione TinyGrad

```bash
cd ~
git clone https://github.com/tinygrad/tinygrad.git
cd tinygrad
```

### Verifica GPU

```bash
# Test che la GPU AMD sia riconosciuta
cd ~/tinygrad
DEBUG=2 AMD=1 /opt/homebrew/bin/python3.11 -c "from tinygrad import Device; print('GPU AMD OK')"
```

**Output atteso:**
```
am remote:0: mode1 reset
am remote:0: AM_SOC initialized
...firmware loading...
am remote:0: boot done
AMDDevice: opening 0 with target (11, 0, 0) arch gfx1100
GPU AMD OK
```

### Verifica Hardware

```bash
system_profiler SPDisplaysDataType | grep -A5 "External GPU"
```

---

## Modelli Disponibili

### Modelli Supportati da TinyGrad

| Modello | Size | VRAM | Velocit√† | Comando |
|---------|------|------|----------|---------|
| `qwen3:0.6b` | 0.6B | ~1GB | ~30 tok/s | `--model qwen3:0.6b` |
| `llama3.2:1b` | 1B | ~2GB | ~15 tok/s | `--model llama3.2:1b` |
| `llama3.2:1b-q4` | 1B | ~1GB | ~20 tok/s | `--model llama3.2:1b-q4` |
| `qwen3:1.7b` | 1.7B | ~3GB | ~12 tok/s | `--model qwen3:1.7b` |
| `llama3.2:3b` | 3B | ~4GB | ~10 tok/s | `--model llama3.2:3b` |
| `llama3.2:3b-f16` | 3B | ~6GB | ~8 tok/s | `--model llama3.2:3b-f16` |
| `qwen2.5:7b` | 7B | ~5GB | ~3 tok/s | `--model qwen2.5:7b` |
| `qwen3:8b` | 8B | ~8GB | ~5 tok/s | `--model qwen3:8b` |
| `llama3.1:8b` | 8B | ~8GB | ~5 tok/s | `--model llama3.1:8b` |
| **`qwen2.5:14b`** | **14B** | **~9GB** | **~2 tok/s** | `--model qwen2.5:14b` |
| `olmoe` | MoE | ~7GB | ~3 tok/s | `--model olmoe` |
| ~~`qwen3:30b-a3b`~~ | ~~30B MoE~~ | ~~~20GB~~ | ~~SCONSIGLIATO~~ | ~~MoE pessimo su eGPU~~ |

### Raccomandazione per VRAM

| VRAM Disponibile | Modello Consigliato |
|------------------|---------------------|
| 8GB | `llama3.1:8b` o `qwen3:8b` |
| 16GB | `llama3.1:8b` con context lungo |
| 20GB (7900 XT) | **`qwen2.5:14b`** |
| 24GB (7900 XTX) | **`qwen2.5:14b`** con max context |

---

## Raccomandazioni per eGPU (Aggiornato 2026-02-01)

### Modelli CONSIGLIATI per TinyGrad + AMD eGPU

**REGOLA FONDAMENTALE: Solo modelli DENSI, mai MoE!**

| Modello | VRAM Pesi | VRAM Totale | Note |
|---------|-----------|-------------|------|
| **Qwen2.5-14B Q4** | ~9GB | ~16-18GB | ‚úÖ **RACCOMANDATO** - Denso, funziona! |
| **Qwen2.5-7B Q4** | ~5GB | ~8-10GB | ‚úÖ Alternativa pi√π leggera |
| LLaMA 3.1 8B Q4 | ~5GB | ~8-10GB | ‚úÖ Testato, funziona |
| Qwen3 0.6B-8B | ~1-8GB | ~2-12GB | ‚úÖ Densi, funzionano |

### NON USARE (Motivi Tecnici)

| Modello | Problema |
|---------|----------|
| ‚ùå `qwen3:30b-a3b` | **MoE** - Routing dinamico = disastro su eGPU |
| ‚ùå `qwq:32b` | Troppo grande (~18GB solo pesi) |
| ‚ùå Modelli FP16 | Usano il doppio della memoria |
| ‚ùå Modelli split | TinyGrad non supporta GGUF multi-file |
| ‚ùå Mixtral/OLMoE | **MoE** - Stessi problemi di routing |

### Perch√© NO MoE su eGPU?

Mixture of Experts (MoE) richiede:
1. **Routing dinamico** - Seleziona esperti diversi per ogni token
2. **Accessi memoria sparsi** - Pessimo su connessione USB/Thunderbolt
3. **Overhead comunicazione** - Ogni token = multiple roundtrip

Su GPU PCIe nativa: OK (alta bandwidth)
Su eGPU userspace: DISASTRO (latenza USB/TB domina)

### Framework Alternativi (se serve pi√π scelta modelli)

| Framework | Pro | Contro |
|-----------|-----|--------|
| **TinyGrad** | Funziona su macOS AMD! | Pochi modelli |
| vLLM AMD | Pi√π modelli, veloce | Solo Linux |
| MLC-LLM | Ottimizzato AMD | Solo Linux |
| llama.cpp ROCm | Max velocit√† | Solo Linux |

---

## Integrazione ClawdBot

### Lanciare Server TinyGrad

```bash
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --serve 11434
```

Questo espone API su `http://localhost:11434/v1/chat/completions`

### Configurazione ClawdBot

Modifica `~/.clawdbot/clawdbot.json`:

```json
{
  "agents": {
    "defaults": {
      "model": {
        "primary": "radeon/qwen2.5:14b",
        "fallbacks": ["radeon/llama3.1:8b"]
      }
    }
  },
  "models": {
    "mode": "merge",
    "providers": {
      "radeon": {
        "baseUrl": "http://localhost:11434/v1",
        "apiKey": "none",
        "api": "openai-completions",
        "models": [
          {
            "id": "qwen2.5:14b",
            "name": "Qwen3 30B (Radeon TinyGrad)",
            "reasoning": false,
            "input": ["text"],
            "cost": {"input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0},
            "contextWindow": 32768,
            "maxTokens": 8192
          },
          {
            "id": "llama3.1:8b",
            "name": "LLaMA 3.1 8B (Radeon TinyGrad)",
            "reasoning": false,
            "input": ["text"],
            "cost": {"input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0},
            "contextWindow": 32768,
            "maxTokens": 8192
          }
        ]
      }
    }
  }
}
```

### Test API

```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:14b",
    "messages": [{"role": "user", "content": "Ciao!"}],
    "stream": false
  }'
```

---

## Tool Calling

### Il Problema

ClawdBot richiede **tool calling nativo** - il modello deve rispondere con:

```json
{"tool_calls": [{"function": {"name": "calculator", "arguments": {"x": "5+3"}}}]}
```

NON con JSON nel content:
```json
{"content": "{\"name\": \"calculator\", \"arguments\": ...}"}
```

### Stato Attuale

| Framework | Tool Calling |
|-----------|--------------|
| TinyGrad | ‚ö†Ô∏è DA VERIFICARE |
| Ollama | ‚úÖ Supportato (ma non funziona su AMD macOS) |

### TODO

1. Testare se TinyGrad server supporta tool calling
2. Se non supporta, valutare workaround (parsing JSON dal content)

### Test Tool Calling

```bash
curl -s http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:14b",
    "messages": [{"role": "user", "content": "Calculate 5+3 using the calculator tool"}],
    "tools": [{
      "type": "function",
      "function": {
        "name": "calculator",
        "description": "Performs math calculations",
        "parameters": {
          "type": "object",
          "properties": {"expression": {"type": "string"}},
          "required": ["expression"]
        }
      }
    }]
  }' | python3 -c "
import sys,json
d = json.load(sys.stdin)
msg = d['choices'][0]['message']
print('Tool calls:', 'YES' if msg.get('tool_calls') else 'NO')
"
```

---

## Troubleshooting

### GPU Non Trovata

1. Verifica eGPU collegata via Thunderbolt
2. `system_profiler SPDisplaysDataType | grep -A5 "External GPU"`
3. Riavvia terminale

### "lock file in use"

Un altro processo sta usando la GPU:
```bash
pkill -f "python.*AMD=1"
```

### Lento al Primo Avvio

Normale - TinyGrad compila i kernel LLVM la prima volta. Successive run pi√π veloci.

### Out of Memory

- Usa modello pi√π piccolo (`llama3.2:1b`)
- Riduci context: `--max_context 2048`

### ClawdBot "context window too small"

ClawdBot richiede minimo 16k context. TinyGrad dovrebbe supportarlo di default con modelli grandi.

---

## Comandi Utili

### Variabili Ambiente

| Variabile | Valore | Descrizione |
|-----------|--------|-------------|
| `AMD` | `1` | Abilita backend AMD |
| `AMD_LLVM` | `1` | Usa LLVM homebrew |
| `PYTHONPATH` | `.` | Include moduli TinyGrad |
| `DEBUG` | `1-7` | Verbosit√† debug |

### Script di Lancio

Crea `~/start-radeon-llm.sh`:

```bash
#!/bin/bash
cd ~/tinygrad
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 \
  tinygrad/apps/llm.py --model qwen2.5:14b --serve 11434
```

```bash
chmod +x ~/start-radeon-llm.sh
```

### Path Importanti

| Path | Descrizione |
|------|-------------|
| `~/tinygrad` | Clone TinyGrad |
| `~/tinygrad/tinygrad/apps/llm.py` | Server LLM |
| `~/start-radeon-llm.sh` | Script avvio |
| `~/.clawdbot/clawdbot.json` | Config ClawdBot |

---

## Fonti

- [TinyGrad GitHub](https://github.com/tinygrad/tinygrad)
- [TinyGrad LLM App](https://github.com/tinygrad/tinygrad/blob/master/tinygrad/apps/llm.py)
- [AMD GPU Performance for LLM](https://valohai.com/blog/amd-gpu-performance-for-llm-inference/)
- [Ollama Tool Calling Docs](https://docs.ollama.com/capabilities/tool-calling)

---

## NVIDIA RTX 5060 Ti - NON FUNZIONA (Analisi 2026-02-01)

### Status: ‚ùå IN ATTESA FIRMWARE GSP ‚â•575

**La RTX 5060 Ti (Blackwell/GB206) NON FUNZIONA su macOS via eGPU.**

N√© con TinyGrad, n√© con nessun altro metodo. Serve firmware GSP ‚â•575 che **non √® ancora pubblico**.

---

### Il Problema

| Aspetto | Dettaglio |
|---------|-----------|
| **GPU** | NVIDIA RTX 5060 Ti 16GB (GB206/Blackwell) |
| **Connessione** | USB4/Thunderbolt via dock ADT-UT3G o Razer Core X |
| **Errore** | `GSP_INIT_DONE returns NV_ERR_TIMEOUT (result=101)` |
| **Causa** | Firmware GSP 570.144 non supporta Blackwell via eGPU |
| **Soluzione** | Aspettare firmware GSP ‚â•575 (non ancora nel repo pubblico) |

---

### Issue Aperti su TinyGrad

- **[tinygrad#14334](https://github.com/tinygrad/tinygrad/issues/14334)** - RTX 5060 Ti GSP init timeout on macOS M1
- **[tinygrad#14338](https://github.com/tinygrad/tinygrad/issues/14338)** - RTX 5060 Ti fails GSP initialization over USB4/Thunderbolt

### Conferma dal Team TinyGrad

**nimlgen** (membro team tinygrad) ha confermato:
> "5060ti is not supported in gsp 570.144. gsp needs to be updated to >=575"

---

### Sequenza Errore GSP

```
0.0s - GSP FMC/COT boots successfully
0.0s - RPC queues initialize (writePtr=1)
0.5s - Display Engine initialization starts (NOCAT type 2)
0.6s - FBFLCN error: UNRECOGNIZED_CLIENT -> HUBCLIENT_CE0 -> HUBCLIENT_VIP
4.6s - Internal timeout
4.6s - GSP_INIT_DONE returns NV_ERR_TIMEOUT (result=101)
```

---

### Firmware nel Repo Pubblico

```
nvidia/gb202/gsp/
‚îú‚îÄ‚îÄ bootloader-570.144.bin
‚îú‚îÄ‚îÄ fmc-570.144.bin
‚îî‚îÄ‚îÄ (NO gsp-*.bin per Blackwell!)
```

Il firmware 575+ necessario **non esiste ancora** nel repo [NVIDIA/linux-firmware](https://github.com/NVIDIA/linux-firmware).

---

### Cosa NON Funziona

| Tentativo | Risultato |
|-----------|-----------|
| Registry keys (RMInitializeHeadless, etc.) | ‚ùå Stesso timeout |
| FMC regkeys (0x0 - 0xFF) | ‚ùå Stesso timeout |
| BDF values diversi | ‚ùå Stesso timeout |
| NV_NAK=1 (LLVM compiler) | ‚ùå Serve comunque GSP per init |
| Patch FRTS/WPR dinamici | ‚ùå Regressione |
| Firmware 575.64 | ‚è≥ Non disponibile pubblicamente |

---

### Arto Bendiken (@bendiken) - NON √à Riuscito

Tweet virale del 21 Nov 2025: ["Late night hacking on making an RTX 5060 Ti work..."](https://x.com/bendiken/status/1992139703232188771)

**ATTENZIONE:** Il tweet sembra mostrare successo, ma leggendo i commenti:

1. **"Late night hacking on MAKING it work"** - Stava PROVANDO, non aveva fatto funzionare
2. **Nessun follow-up "funziona!"** - Mai postato conferma di successo
3. **Suoi commenti:**
   - "Nobody's tested that yet that I've seen, though"
   - "Not at the moment. You'd want an RTX 30xx or newer"
4. **Libreria creata:** [artob/libegpu](https://github.com/artob/libegpu) - Solo per ENUMERARE eGPU, non per usarle

**Conclusione:** Nessuno ha fatto funzionare la RTX 5060 Ti via eGPU su Mac (M1 o M4).

---

### Cosa Funziona (RTX 30/40 series)

TinyGrad supporta **RTX 30xx e 40xx** via USB4/Thunderbolt con firmware 570.144.

| GPU | Architettura | Boot Path | Status |
|-----|--------------|-----------|--------|
| RTX 3090 | Ampere (GA102) | Standard GSP | ‚úÖ Funziona |
| RTX 4090 | Ada (AD102) | Standard GSP | ‚úÖ Funziona |
| **RTX 5060 Ti** | **Blackwell (GB206)** | **FMC/COT** | **‚ùå Aspetta FW 575+** |
| RTX 5070 | Blackwell (GB205) | FMC/COT | ‚ùå Aspetta FW 575+ |
| RTX 5080 | Blackwell (GB203) | FMC/COT | ‚ùå Aspetta FW 575+ |
| RTX 5090 | Blackwell (GB202) | FMC/COT | ‚ùå Aspetta FW 575+ |

---

### Issue NVIDIA Correlati

- [NVIDIA/open-gpu-kernel-modules#979](https://github.com/NVIDIA/open-gpu-kernel-modules/issues/979) - RTX 5080 TB5 eGPU hard locks
- [NVIDIA/open-gpu-kernel-modules#981](https://github.com/NVIDIA/open-gpu-kernel-modules/pull/981) - TB eGPU detection improvements (CHIUSO)

---

### Prossimi Passi

1. **Monitorare** il repo [NVIDIA/linux-firmware](https://github.com/NVIDIA/linux-firmware) per firmware 575+
2. **Monitorare** issue [tinygrad#14338](https://github.com/tinygrad/tinygrad/issues/14338) per aggiornamenti
3. **Usare AMD RX 7900 XTX/XT** nel frattempo (funziona perfettamente!)

---

### Hardware Testato (NON Funzionante)

| Componente | Dettaglio |
|------------|-----------|
| Mac | MacBook Pro M1 / MacBook Pro M4 |
| GPU | NVIDIA RTX 5060 Ti 16GB |
| Dock | ADT-UT3G / Razer Core X V2 |
| Driver | TinyGPU (socket /tmp/tinygpu.sock OK) |
| Firmware | 570.144 (unico disponibile) |
| SIP | Disabilitato |

---

## üî¨ TEST SESSION 2026-02-01 (Claude Code)

### Obiettivo
Testare qwen2.5:14b sulla RX 7900 XT (20GB VRAM) con la config raccomandata da ChatGPT.

### Ambiente
- **GPU:** AMD Radeon RX 7900 XT (20GB VRAM) via Thunderbolt
- **Driver:** TinyGPU userspace driver
- **TinyGrad:** ~/tinygrad (con patch Float16 applicato)
- **Python:** 3.11

### Test Eseguiti

| Test | Risultato | Note |
|------|-----------|------|
| GPU Init | ‚úÖ OK | `Device['AMD']` funziona |
| Allocazione 30GB | ‚úÖ OK | Usa RAM come fallback oltre 20GB |
| qwen3:0.6b | ‚úÖ OK | ~1GB, genera token |
| qwen2.5:7b | ‚ö†Ô∏è Garbage | Carica ma output corrotto ("p√µe..") |
| qwen2.5:14b load | ‚ùå OOM | Arriva a ~21GB poi fallisce |
| qwen2.5:14b realize=False | ‚úÖ Parziale | 4 batch (200 tensors) poi OOM |
| llm.py server | ‚ùå Crash | Server termina dopo pochi secondi |

### Analisi Problema OOM

**Errore tipico:**
```
MemoryError: Failed to allocate memory. (total allocation size=0x8700000, current try=(4096, 4096))
```

**Causa:** Il modello qwen2.5:14b Q4_K_M (8.4GB file) richiede ~21GB durante realize:
- Pesi Q4: ~9GB
- De-quantizzazione temporanea: ~?GB
- KV cache (1024 ctx): ~?GB
- Totale > 20GB disponibili

**Tentativo di soluzione:**
- `max_context=256` ‚Üí stesso errore
- `realize=False` + batch realize ‚Üí arriva a 200 tensors poi OOM
- Server llm.py ‚Üí crash

### Conclusioni

1. **qwen2.5:14b NON entra nei 20GB** della RX 7900 XT con TinyGrad attuale
2. **qwen2.5:7b carica** ma ha output corrotto (possibile bug tokenizer)
3. **Modelli piccoli (0.6b-3b) funzionano** perfettamente
4. **Serve RX 7900 XTX (24GB)** o ottimizzazioni TinyGrad per il 14b

### Prossimi Passi

1. **Testare con meno context** (128 token) per vedere se entra
2. **Investigare output corrotto** su qwen2.5:7b
3. **Aprire issue TinyGrad** per OOM su modelli 14B+
4. **Considerare llama3.1:8b** come alternativa (testato funzionante 2026-01-28)

### Modelli Confermati Funzionanti

| Modello | VRAM | Output | Velocit√† |
|---------|------|--------|----------|
| qwen3:0.6b | ~1GB | ‚úÖ OK | ~30 tok/s |
| llama3.1:8b Q4 | ~5GB | ‚úÖ OK | ~0.64 tok/s |
| GPT-2 XL | ~6.5GB | ‚úÖ OK | ~2.9 tok/s |

### Modelli NON Funzionanti

| Modello | Problema |
|---------|----------|
| qwen2.5:14b | OOM (>20GB) |
| qwen2.5:7b | Output garbage |
| qwq-32b | Troppo grande (18GB pesi) |

---

## Changelog

- **v1.3 (2026-02-01)**: Aggiunta sessione test Claude Code - qwen2.5:14b OOM, qwen2.5:7b garbage, analisi limiti VRAM
- **v1.2 (2026-02-01)**: Aggiunta sezione NVIDIA RTX 5060 Ti - analisi completa del fallimento, riferimenti issue, debunking tweet Arto Bendiken
- **v1.1 (2026-02-01)**: Aggiunta sezione HANDOVER dettagliata con step-by-step, errori commessi, task pendenti
- **v1.0 (2026-02-01)**: Consolidamento BIBBIA-RADEON + HANDOVER + CLAWDBOT-SETUP + raccomandazioni Grok

---

## üî¨ TEST SESSION v1.4 - 2026-02-01 (BUG FIX ATTENTION BIAS)

### üêõ BUG CRITICO TROVATO E FIXATO

**Problema:** Qwen2.5 (tutte le versioni) produceva **garbage output** ("p√µe..", caratteri casuali).

**Causa Root:** In `/Users/mattia/tinygrad/tinygrad/apps/llm.py`, le proiezioni attention erano hardcoded con `bias=False`:

```python
# PRIMA (linee 97-99) - BROKEN per Qwen2.5
self.attn_q      = nn.Linear(dim, q_proj_out,  bias=False)
self.attn_k      = nn.Linear(dim, kv_proj_out, bias=False)
self.attn_v      = nn.Linear(dim, kv_proj_out, bias=False)
```

Ma il modello Qwen2.5 GGUF **contiene tensori bias** che venivano **ignorati**:
- `blk.0.attn_q.bias: (3584,)`
- `blk.0.attn_k.bias: (512,)`
- `blk.0.attn_v.bias: (512,)`

**Architetture a confronto:**
| Architettura | Attention Bias | Modelli |
|--------------|----------------|---------|
| LLaMA | ‚ùå NO | LLaMA 1/2/3.x |
| Qwen3 | ‚ùå NO | Qwen3 0.6B-8B |
| **Qwen2/Qwen2.5** | **‚úÖ S√å** | Qwen2.5 7B/14B |

### ‚úÖ FIX APPLICATO

**File modificato:** `/Users/mattia/tinygrad/tinygrad/apps/llm.py`

**Modifica 1 - TransformerBlock.__init__ (linea 86):**
```python
# DOPO - Aggiunto parametro attn_bias
def __init__(self, dim:int, hidden_dim:int, n_heads:int, n_kv_heads:int, norm_eps:float, head_dim:int, rope_theta:float,
             max_context:int=0, qk_norm:int=0, num_experts:int=0, num_experts_per_tok:int=0, attn_bias:bool=False):
```

**Modifica 2 - Proiezioni attention (linee 97-99):**
```python
# DOPO - Usa attn_bias invece di hardcoded False
self.attn_q      = nn.Linear(dim, q_proj_out,  bias=attn_bias)
self.attn_k      = nn.Linear(dim, kv_proj_out, bias=attn_bias)
self.attn_v      = nn.Linear(dim, kv_proj_out, bias=attn_bias)
```

**Modifica 3 - Transformer.__init__ (linea 163):**
```python
# DOPO - Propaga attn_bias ai blocchi
def __init__(self, *, num_blocks, dim, hidden_dim, n_heads, n_kv_heads, norm_eps, vocab_size, head_dim:int, rope_theta:float,
             max_context:int=0, qk_norm:int=0, num_experts:int=0, num_experts_per_tok:int=0, attn_bias:bool=False):
```

**Modifica 4 - from_gguf detection (linea 212):**
```python
# DOPO - Auto-detect bias da state_dict
attn_bias='blk.0.attn_q.bias' in state_dict
```

### üìä SCOPERTA CRITICA: DE-QUANTIZZAZIONE TinyGrad

**IL VERO PROBLEMA CON MODELLI 14B+:**

TinyGrad **de-quantizza TUTTI i pesi a FP16** durante il caricamento! Il codice in `llm.py` linea 189:

```python
state_dict = {k:v.cast('float16') if getenv("HALF", 1) else v for k,v in state_dict.items()}
```

**Conseguenza:**
| Modello | File GGUF | In VRAM (FP16) | Risultato |
|---------|-----------|----------------|-----------|
| Qwen2.5-7B Q4 | ~4GB | ~15GB | ‚ö†Ô∏è Carica, 5GB liberi |
| Qwen2.5-14B Q4 | ~9GB | **~30GB** | ‚ùå OOM (>20GB) |

**Questo significa che:**
- I modelli Q4 NON risparmiano VRAM in TinyGrad
- Servono ~2x la dimensione teorica
- 14B Q4 (dovrebbe essere ~8-9GB) diventa ~30GB ‚Üí OOM

**Per implementare vero quantized inference serve:**
- Matmul quantizzato che de-quantizza on-the-fly per blocco
- Simile a llama.cpp ma richiede modifiche profonde a TinyGrad
- Progetto complesso ma fattibile (TinyGrad √® open source)

### üîç STATO MODELLI SSD

**Path:** `/Volumes/DATI-SSD/llm-models/`

**ATTENZIONE: LA CARTELLA √à VUOTA!**

Tutti i modelli sono stati cancellati/persi:
- ‚ùå `Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf` - NON PRESENTE
- ‚ùå `Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf` - NON PRESENTE
- ‚ùå `Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf` - NON PRESENTE
- ‚ùå `qwq-32b-q4_k_m.gguf` - Cancellato (troppo grande)
- ‚ùå `Qwen2.5-14B-Instruct-Q4_K_M.gguf` - Download fallito (0B)

**Per testare il fix, bisogna ri-scaricare almeno qwen2.5:7b:**
```bash
curl -L -o /Volumes/DATI-SSD/llm-models/Qwen2.5-7B-Instruct-Q4_K_M.gguf \
  "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_M.gguf"
```

### ‚ö†Ô∏è DUBBIO SU LLAMA + AMD

**CLAUDE.md documenta:**
```
### LLaMA 3.1 8B su AMD (FUNZIONA! - 2026-01-28)
PYTHONPATH=. AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 ...
**Performance:** ~0.64 tok/s
```

**Ma il user dubita che fosse davvero su AMD Radeon.**

**Possibilit√†:**
1. Test eseguito su **Metal** (Apple M1) invece che AMD
2. Il modello era gi√† in cache e Metal l'ha intercettato
3. `AMD=1` era settato ma TinyGrad usava comunque Metal come fallback

**Verifica necessaria:**
```bash
# Con DEBUG=2, deve mostrare:
# "AMDDevice: opening 0 with target (11, 0, 0) arch gfx1100"
cd ~/tinygrad
DEBUG=2 AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 -c "
from tinygrad import Device
print(Device.DEFAULT)
print(Device['AMD'])
"
```

Se mostra `Metal` o `CPU` invece di `AMD`, allora i test precedenti NON erano su AMD.

### üìÅ FILE CRITICI E PATH

| File | Path Assoluto | Descrizione |
|------|---------------|-------------|
| **llm.py (MODIFICATO)** | `/Users/mattia/tinygrad/tinygrad/apps/llm.py` | Server LLM con fix attn_bias |
| **state.py** | `/Users/mattia/tinygrad/tinygrad/nn/state.py` | GGUF loader (ha gi√† Float16 patch) |
| **memory.py** | `/Users/mattia/tinygrad/tinygrad/runtime/support/memory.py` | Dove origina OOM |
| **ops_amd.py** | `/Users/mattia/tinygrad/tinygrad/runtime/ops_amd.py` | AMD device driver |
| **amdev.py** | `/Users/mattia/tinygrad/tinygrad/runtime/support/am/amdev.py` | AMD device manager |
| **Modelli** | `/Volumes/DATI-SSD/llm-models/` | **VUOTA** - modelli da ri-scaricare |
| **Test log** | `/tmp/qwen14b-test.log` | Log errore OOM |

### üîß COMANDI UTILI

**Reset GPU memory prima di ogni test:**
```bash
pkill -9 -f "python.*AMD"
sleep 2
```

**Test AMD device:**
```bash
cd ~/tinygrad
DEBUG=2 AMD=1 /opt/homebrew/bin/python3.11 -c "from tinygrad import Device; Device['AMD']"
```

**Download modello qwen2.5:7b:**
```bash
curl -L -o /Volumes/DATI-SSD/llm-models/Qwen2.5-7B-Instruct-Q4_K_M.gguf \
  "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_M.gguf"
```

**Test qwen2.5:7b con fix (dopo download):**
```bash
cd ~/tinygrad
AMD=1 AMD_LLVM=1 /opt/homebrew/bin/python3.11 -c "
import os, sys
from tinygrad import Tensor, dtypes, Device
sys.path.insert(0, 'tinygrad/apps')
from llm import Transformer, SimpleTokenizer

model_path = '/Volumes/DATI-SSD/llm-models/Qwen2.5-7B-Instruct-Q4_K_M.gguf'
gguf_tensor = Tensor.empty(os.stat(model_path).st_size, dtype=dtypes.uint8, device=f'disk:{model_path}')
model, kv = Transformer.from_gguf(gguf_tensor.to(None), max_context=256)
tokenizer = SimpleTokenizer.from_gguf_kv(kv)
print(f'Model loaded! arch={kv[\"general.architecture\"]}')

prompt = 'What is 2+2?'
formatted = tokenizer.role('user') + tokenizer.encode(prompt) + tokenizer.end_turn(151645) + tokenizer.role('assistant')
gen = model.generate(formatted, 0)
output = [next(gen) for _ in range(20)]
print(f'Output: {tokenizer.decode(output)}')
"
```

### üìã PROSSIMI PASSI

1. **Ri-scaricare qwen2.5:7b** sulla SSD
2. **Testare il fix** - verificare che non produca pi√π garbage
3. **Verificare LLaMA su AMD** - confermare se i test precedenti erano realmente su AMD
4. **Considerare PR a TinyGrad** - il fix attn_bias √® generico e utile

### üö® ERRORI OOM TIPICI

```
MemoryError: Can't allocate 4096 bytes
MemoryError: Failed to allocate memory. (total allocation size=0x8700000, current try=(4096, 4096))
```

**Significato:** La GPU ha esaurito la VRAM (~20GB). Causa:
- Modello troppo grande dopo de-quantizzazione
- KV cache + pesi superano 20GB
- Memory leak da test precedenti (soluzione: `pkill -9 -f "python.*AMD"`)

---

## Changelog

- **v1.4 (2026-02-01)**: Bug fix attention bias per Qwen2.5 - trovata causa root garbage output, fix applicato a llm.py, documentata de-quantizzazione TinyGrad, SSD modelli vuota
- **v1.3 (2026-02-01)**: Aggiunta sessione test Claude Code - qwen2.5:14b OOM, qwen2.5:7b garbage, analisi limiti VRAM
- **v1.2 (2026-02-01)**: Aggiunta sezione NVIDIA RTX 5060 Ti - analisi completa del fallimento, riferimenti issue, debunking tweet Arto Bendiken
- **v1.1 (2026-02-01)**: Aggiunta sezione HANDOVER dettagliata con step-by-step, errori commessi, task pendenti
- **v1.0 (2026-02-01)**: Consolidamento BIBBIA-RADEON + HANDOVER + CLAWDBOT-SETUP + raccomandazioni Grok

---

*Documento consolidato da: BIBBIA-RADEON.md, HANDOVER-CLAWDBOT-RADEON.md, CLAWDBOT-RADEON-SETUP.md*
*Questo √® l'UNICO documento di riferimento. Non creare altri file.*
